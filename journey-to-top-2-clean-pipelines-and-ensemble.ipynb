{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/titanic/gender_submission.csv\n",
      "/kaggle/input/titanic/test.csv\n",
      "/kaggle/input/titanic/train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "<p><h1>Finding survivors of the titanic tragedy</h1></p>\n",
    "<h4>Countering data contamination and attempt at a clean code</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "<p>In this notebook:<ul>\n",
    "    <li>We will not be doing a detailed EDA since there are many EDA based notebooks out there on this particular problem (titanic survivors)</li>\n",
    "    <li>We will enlist and explain the originally existing features , the features that are extracted , and the features that are removed</li>  \n",
    "    <li>We will build an xgboost classifier and a logistic regression classifier using pipelines</li>\n",
    "    <li>We will evaluate our models using cross validation</li>\n",
    "    <li>We will build a voting ensemble classifier using the above models</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "We will mainly emphasize on the optimal usage of pipelines to reduce risks of data leakage . "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "<h1>DATA PREPROCESSING</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "let us first import out libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "from category_encoders.leave_one_out import LeaveOneOutEncoder\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "from scipy.stats import boxcox\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold,cross_val_score,GridSearchCV\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import math\n",
    "import sklearn.pipeline as pip\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import quantile_transform\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "<h3>Let us now look at our features</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "<h3>Originally existing features : (description for these features can be found in the data tab of the titanic competition)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/titanic/train.csv')\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "<p><h3>Features that are extracted : </h3></p>\n",
    "<ul><li>'Deck' :    The alphabet (prefix) from the 'Cabin' feature is extracted  as deck .</li>\n",
    "    <li>'Title' :    The Title alone is extracted from the 'Name' feature and is binned into the major categories .</li>\n",
    "    <li>'Family_Size :    = 'SibSp' + 'Parch' .</li>\n",
    "    <li>'Fare_Bin' :    'Fare_Bin' is a feature where the 'Fare' column is binned into different categories (the number and boundaries of these categories were decided while analaysing the data)</li>\n",
    "    <li>'Age_grp' :    This is the binned form of the 'Age' column and binnning is done using clustering analysis. The optimal no of clusters (k) of age was found using the sillhouette method (find the maxima on the sillhouette curve) . These k clusters of age were then analyzed and their boundaries were extracted . these boundaries were used to map and bin the 'Age' column into different k different categories . </li>\n",
    "    <li>'Fare_per_person' :    = 'Fare' / ('Family_Size'+1)</li>\n",
    "    <li>'Age*Class' : = 'Age' x 'Class' </li>\n",
    "    <li>'Family_Survival' : This feature is extracted based on the assumption that a family either has the same 'Last_name'(extracted from the 'Name' feature) or the same 'Ticket' . This feature states : if any one person from a family survives , the 'Family_Survival' for all the passengers in the family will be = 1 . Subsequently , if any one person from a family dies , the 'Family_Survival' for the whole family will = 0 . All passengers start out with a basic 'Family_Survival' of 0.5 . </li>\n",
    "    <li>'is_alone' : If 'Family_Size' == 0 , then 'is_alone' = 1</li>\n",
    "    <li>'has_cabin' : If 'Deck'!='Unknown' , then 'has_cabin' = 1</li>\n",
    "    <li>'is_3rdclass' : If 'Pclass' == 3 , then 'is_3rdclass' = 1</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "<p><h3>features that will be removed : </h3></p>\n",
    "<ul>\n",
    "<li>'PassengerId'</li>\n",
    "<li>'Name'</li>\n",
    "<li>'Ticket'</li>\n",
    "<li>'Cabin'</li>\n",
    "<li>'Last_Name'</li>\n",
    "</ul>\n",
    "<p>These features are almost unique for each passenger and advanced processing must be done to make use of these features . Therefore , for simplicity we drop these features . </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "<p><h3>Over the course of data preprocessing many functions are used that will help in extracting features</h3></p> \n",
    "<p><h3>All the functions that are used for processing the data and extracting the features are given below  : </h3></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"----------------------DATA PREPROCESSING-----------------------\"\"\"\n",
    "\"\"\"method used to extract 'Title' \"\"\"\n",
    "def substrings_in_string(big_string, substrings):\n",
    "    for substring in substrings:\n",
    "        if big_string.find(substring) != -1:\n",
    "            return substring\n",
    "    print (big_string)\n",
    "    return np.nan\n",
    "\n",
    "\"\"\"PHASE 1 : Extracting 'Deck' , 'Title' , 'Family_Size'\"\"\"\n",
    "def phase1clean(df):\n",
    "    \n",
    "    #setting silly values to nan\n",
    "    df.Fare = df.Fare.map(lambda x: np.nan if x==0 else x)\n",
    "    \n",
    "    #Special case for cabins as 'nan' may be signal\n",
    "    df.Cabin = df.Cabin.fillna('Unknown')    \n",
    "    cabin_list = ['A', 'B', 'C', 'D', 'E', 'F', 'T', 'G', 'Unknown']\n",
    "    df['Deck']=df['Cabin'].map(lambda x: substrings_in_string(x, cabin_list))\n",
    "        \n",
    "    \n",
    "    #creating a title column from name\n",
    "    title_list=['Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev',\n",
    "                'Dr', 'Ms', 'Mlle','Col', 'Capt', 'Mme', 'Countess',\n",
    "                'Don', 'Jonkheer']\n",
    "    \n",
    "    df['Title']=df['Name'].map(lambda x: substrings_in_string(x, title_list))\n",
    "    \n",
    "    #replacing all titles with mr, mrs, miss, master\n",
    "    def replace_titles(x):\n",
    "        title=x['Title']\n",
    "        if title in ['Countess','Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']:\n",
    "            return 'Rare'\n",
    "        elif title in ['Countess', 'Mme']:\n",
    "            return 'Mrs'\n",
    "        elif title in ['Mlle', 'Ms']:\n",
    "            return 'Miss'\n",
    "        elif title =='Dr':\n",
    "            if x['Sex']=='Male':\n",
    "                return 'Mr'\n",
    "            else:\n",
    "                return 'Mrs'\n",
    "        else:\n",
    "            return title\n",
    "    \n",
    "    df['Title']=df.apply(replace_titles, axis=1)\n",
    "    \n",
    "    #Creating new family_size column\n",
    "    df['Family_Size']=df['SibSp']+df['Parch']\n",
    "    \n",
    "    return df\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"Creating 'Fare_Bin' : Binning fare using manually set boundaries\"\"\"\n",
    "def fare_grouping(x):\n",
    "    # Ranging and grouping Fare using historical data\n",
    "    bins = [-1, 7.91, 14.454, 31, 99, 250, np.inf]\n",
    "    names = ['a', 'b','c', 'd', 'e', 'f']\n",
    "    names = [1, 2, 3, 4, 5, 6]\n",
    "    x['Fare_Bin'] = pd.cut(x['Fare'], bins ,labels=names).astype('int')\n",
    "    dict_age={1 : 'a' , 2 : 'b' , 3 : 'c' , 4 : 'd' , 5: 'e' , 6 : 'f'}\n",
    "    x['Fare_Bin']=x['Fare_Bin'].map(dict_age)\n",
    "    \"\"\"visualizing fares\"\"\"\n",
    "    # sns.factorplot(x=\"Fare_Bin\", data=x , kind=\"count\",size=6, aspect=.7)\n",
    "    # plt.show()\n",
    "    # sns.scatterplot(x=\"PassengerId\",y=\"fare_gauss\",hue=\"Fare_Bin\",data=x,palette='RdYlGn' , legend='full')\n",
    "    # plt.show()\n",
    "    # sns.distplot(x['Age'],axlabel='training set');\n",
    "    # plt.show()\n",
    "    return x\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"Removing all missing values\"\"\"\n",
    "def fill_nan(x):\n",
    "    \"\"\"filling age nan's\"\"\"\n",
    "    null_ind=x.loc[x['Age'].isnull(),:].index\n",
    "    null_count=x.loc[x['Age'].isnull(),]['PassengerId'].count()\n",
    "    num_ages=x.groupby('Title')['Age'].mean().to_dict()\n",
    "    x.loc[x['Age'].isnull(),'Age']=x.loc[x['Age'].isnull(),'Title'].map(num_ages)\n",
    "\n",
    "    \n",
    "    \"\"\"filling fare and binning fare\"\"\"\n",
    "    null_ind=x.loc[x['Fare'].isnull(),:].index\n",
    "    null_count=x.loc[x['Fare'].isnull(),]['PassengerId'].count()\n",
    "    num_fare=x.groupby('Pclass')['Fare'].mean().to_dict()\n",
    "    x.loc[x['Fare'].isnull(),'Fare']=x.loc[x['Fare'].isnull(),'Pclass'].map(num_fare)\n",
    "    fare_grouping(x)\n",
    "    \n",
    "    \"\"\"removing embarked nan's\"\"\"\n",
    "    f_index=x[x['Embarked'].isnull()].index\n",
    "    x=x.drop(f_index,axis=0)\n",
    "    \n",
    "    return x\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"Creating 'Age_Grp' : Binning age using silhouette method and clustering \"\"\"\n",
    "def age_grouping(x):\n",
    "    \"\"\"visualizing age feature\"\"\"\n",
    "    # sns.distplot(x['Age'],axlabel='training set');\n",
    "    # plt.show()\n",
    "    # sns.factorplot(x=\"Age_Grp\", data=x , kind=\"count\",size=6, aspect=.7)\n",
    "    # plt.show()\n",
    "    # sns.factorplot(x=\"Age_Grp\",col=\"Survived\", data=x , kind=\"count\",size=6, aspect=.7)\n",
    "    # plt.show()\n",
    "    k_data=pd.concat([x['Age'],x['Survived']],axis=1)\n",
    "    k_data.rename( columns={ 0 :'Age' , 1 :'Survived'}, inplace=True )\n",
    "    \"\"\"finding optimal no of clusters using silhouette method\"\"\"\n",
    "    model = KMeans()\n",
    "    sil = []\n",
    "    # dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2\n",
    "    for k in range(2, 9):\n",
    "      kmeans = KMeans(n_clusters = k).fit(k_data)\n",
    "      labels = kmeans.labels_\n",
    "      sil.append(silhouette_score(k_data, labels, metric = 'euclidean'))\n",
    "    plt_data=pd.concat([pd.Series(range(2,9)),pd.Series(sil)],axis=1)\n",
    "    plt_data.rename( columns={ 0 :'clusters' , 1 :'silhouette scores'}, inplace=True )\n",
    "    \"\"\"visualizing the silhouette score plot\"\"\"\n",
    "    # ax = sns.lineplot(x=\"clusters\", y=\"silhouette scores\",\n",
    "    #                   estimator=None, lw=1,\n",
    "    #                   err_style=\"bars\", ci=68, \n",
    "    #                   data=plt_data)\n",
    "    kmeans = KMeans(n_clusters=4, random_state=0).fit(k_data)\n",
    "    labels_=kmeans.labels_\n",
    "    k_data['age_grp']=labels_+1\n",
    "    k_data['Passenger_Id']=x['PassengerId']\n",
    "    \"\"\"visualizing the distribution of passengers of each cluster\"\"\"\n",
    "    # sns.scatterplot(x=\"Passenger_Id\",y=\"Age\",data=k_data,hue=\"age_grp\",palette='viridis' , legend='full')\n",
    "    # plt.show()\n",
    "    # age_grp_survcount=x.loc[x['Survived']==1,:].groupby('Age_Grp')['Survived'].count()\n",
    "    \n",
    "    \"\"\"finding boundaries of the clusters\"\"\"\n",
    "    #age_grp_max=k_data.groupby('Age_Grp').Age.max()\n",
    "    #age_grp_min=k_data.groupby('Age_Grp').Age.min()\n",
    "    \"\"\"making the boundary map for the different clusters \"\"\"\n",
    "    agelist=[] \n",
    "    for i in range(0, 15):\n",
    "        agelist.append('a')\n",
    "    for i in range(15, 29):\n",
    "        agelist.append('b')\n",
    "    for i in range(29, 45): \n",
    "        agelist.append('c')\n",
    "    for i in range(45, 90):\n",
    "        agelist.append('d')\n",
    "    age_dict={v: k for v, k in enumerate(agelist)}\n",
    "    x['Age_Grp']=x['Age'].astype(int).map(age_dict)\n",
    "\n",
    "    return x,age_dict\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"Creating 'Family_Survived' feature\"\"\"\n",
    "def survived_fams(df):\n",
    "    # A function working on family survival rate using last names and ticket features\n",
    "    df['Last_Name'] = df['Name'].apply(\n",
    "        lambda df: str.split(df, \",\")[0])\n",
    "    \n",
    "    # Adding new feature: 'Survived'\n",
    "    default_survival_rate = 0.5\n",
    "    df['Family_Survival'] = default_survival_rate\n",
    "    \n",
    "    for grp, grp_df in df[['Survived', 'Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId','SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n",
    "    \n",
    "        if (len(grp_df) != 1):\n",
    "            # A Family group is found.\n",
    "            for ind, row in grp_df.iterrows():\n",
    "                smax = grp_df.drop(ind)['Survived'].max()\n",
    "                smin = grp_df.drop(ind)['Survived'].min()\n",
    "                passID = row['PassengerId']\n",
    "                if (smax == 1.0):\n",
    "                    df.loc[df['PassengerId'] ==\n",
    "                                  passID, 'Family_Survival'] = 1\n",
    "                elif (smin == 0.0):\n",
    "                    df.loc[df['PassengerId'] ==\n",
    "                                  passID, 'Family_Survival'] = 0\n",
    "    \n",
    "    for _, grp_df in df.groupby('Ticket'):\n",
    "        if (len(grp_df) != 1):\n",
    "            for ind, row in grp_df.iterrows():\n",
    "                if (row['Family_Survival'] == 0) | (\n",
    "                        row['Family_Survival'] == 0.5):\n",
    "                    smax = grp_df.drop(ind)['Survived'].max()\n",
    "                    smin = grp_df.drop(ind)['Survived'].min()\n",
    "                    passID = row['PassengerId']\n",
    "                    if (smax == 1.0):\n",
    "                        df.loc[df['PassengerId'] ==\n",
    "                                      passID, 'Family_Survival'] = 1\n",
    "                    elif (smin == 0.0):\n",
    "                        df.loc[df['PassengerId'] ==\n",
    "                                      passID, 'Family_Survival'] = 0\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"Creating 'is_alone' feature\"\"\"\n",
    "def is_alone(x):\n",
    "    fam_list={False : 0 , True : 1}\n",
    "    x['is_alone'] = (x['Family_Size']==0).map(fam_list)\n",
    "    \"\"\"visualization\"\"\"\n",
    "    # sns.factorplot(x=\"is_alone\", data=clean_train_reg , kind=\"count\",size=6, aspect=.7)\n",
    "    # f_1=clean_train_reg[clean_train_reg['Survived']==1].groupby('is_alone')['Survived'].count()\n",
    "    # f_0=clean_train_reg[clean_train_reg['Survived']==0].groupby('is_alone')['Survived'].count()\n",
    "    # f_s=f_1/f_1+f_0\n",
    "    return x\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"Creating 'has_cabin' feature\"\"\"\n",
    "def has_cabin(x):\n",
    "    fam_list={False : 0 , True : 1}\n",
    "    x['has_cabin'] = (x['Deck']!='Unknown').map(fam_list)\n",
    "    \"\"\"visualization\"\"\"\n",
    "    return x\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"Creating 'is_3rdclass' feature\"\"\"    \n",
    "def is_3stclass(x):\n",
    "    class_list={False : 0 , True : 1}\n",
    "    x['is_3rdclass'] = (x['Pclass']== 3 ).map(class_list)\n",
    "    \"\"\"visualization\"\"\"\n",
    "    return x\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"Coverting Pclass.type from 'int' to 'object' \"\"\" \n",
    "def class_categorizer(x):\n",
    "    dict_class={1 : 'a' , 2 : 'b' , 3 : 'c' }\n",
    "    x['Pclass']=x['Pclass'].map(dict_class)\n",
    "    return x\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\"\"\"PHASE 2 : Extracting 'Age_Grp' , 'Fare_Per_Person' , 'Age*Class' , 'Family_Survival' , 'is_alone' , 'has_cabin' , 'is_3rdclass' \"\"\"\n",
    "def phase2clean(train, test):\n",
    "            #data type dictionary\n",
    "    # data_type_dict={'Pclass':'ordinal', 'Sex':'nominal', \n",
    "    #                 'Age':'numeric', \n",
    "    #                 'Fare':'numeric', 'Embarked':'nominal', 'Title':'nominal',\n",
    "    #                 'Deck':'nominal', 'Family_Size':'ordinal'}      \n",
    "    \n",
    "    \n",
    "    #imputing nan values\n",
    "    train=fill_nan(train)\n",
    "    train,age_dict=age_grouping(train)\n",
    "    test=fill_nan(test)\n",
    "    test['Age_Grp']=test['Age'].astype(int).map(age_dict)\n",
    "    \n",
    "    \n",
    "    #Fare per person\n",
    "    for df in [train, test]:\n",
    "        df['Fare_Per_Person']=df['Fare']/(df['Family_Size']+1)\n",
    "    \n",
    "    #Age times class\n",
    "    for df in [train, test]:\n",
    "        df['Age*Class']=df['Age']*df['Pclass']\n",
    "\n",
    "\n",
    "    combined=pd.concat([train,test])\n",
    "    combined=survived_fams(combined)\n",
    "    combined=is_alone(combined)\n",
    "    combined=has_cabin(combined)\n",
    "    combined=is_3stclass(combined)\n",
    "    train=combined.iloc[:len(train),:]\n",
    "    test=combined.iloc[len(train):,:]\n",
    "    test=test.drop(['Survived'],axis=1)\n",
    "    \n",
    "    return [train,test]\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"Container function for all the data preprocessing methods\"\"\"\n",
    "\"\"\"It also handles the removal of the unwanted features ,it also returns the original training and test datasets \"\"\"\n",
    "def get_data():\n",
    "    train_data = pd.read_csv('/kaggle/input/titanic/train.csv')\n",
    "    submit_x =pd.read_csv('/kaggle/input/titanic/test.csv')\n",
    "    original_train = pd.read_csv('/kaggle/input/titanic/train.csv')\n",
    "    original_test = pd.read_csv('/kaggle/input/titanic/test.csv')\n",
    "    x=phase1clean(train_data)\n",
    "    pred_set=phase1clean(submit_x)\n",
    "    x,pred_set=phase2clean(x, pred_set)\n",
    "    x=x.drop(['PassengerId','Name','Ticket','Cabin','Last_Name'],axis=1)\n",
    "    pred_set=pred_set.drop(['PassengerId','Name','Ticket','Cabin','Last_Name'],axis=1)\n",
    "    y = x.Survived\n",
    "    x = x.loc[:,x.columns!='Survived'] \n",
    "    return x,y,pred_set,original_train,original_test\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "<h4>We will now implement the data preprocessing and have a look at out processed data</h4>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 889 entries, 0 to 890\n",
      "Data columns (total 18 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Pclass           889 non-null    int64  \n",
      " 1   Sex              889 non-null    object \n",
      " 2   Age              889 non-null    float64\n",
      " 3   SibSp            889 non-null    int64  \n",
      " 4   Parch            889 non-null    int64  \n",
      " 5   Fare             889 non-null    float64\n",
      " 6   Embarked         889 non-null    object \n",
      " 7   Deck             889 non-null    object \n",
      " 8   Title            889 non-null    object \n",
      " 9   Family_Size      889 non-null    int64  \n",
      " 10  Fare_Bin         889 non-null    object \n",
      " 11  Age_Grp          889 non-null    object \n",
      " 12  Fare_Per_Person  889 non-null    float64\n",
      " 13  Age*Class        889 non-null    float64\n",
      " 14  Family_Survival  889 non-null    float64\n",
      " 15  is_alone         889 non-null    int64  \n",
      " 16  has_cabin        889 non-null    int64  \n",
      " 17  is_3rdclass      889 non-null    int64  \n",
      "dtypes: float64(5), int64(7), object(6)\n",
      "memory usage: 132.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Deck</th>\n",
       "      <th>Title</th>\n",
       "      <th>Family_Size</th>\n",
       "      <th>Fare_Bin</th>\n",
       "      <th>Age_Grp</th>\n",
       "      <th>Fare_Per_Person</th>\n",
       "      <th>Age*Class</th>\n",
       "      <th>Family_Survival</th>\n",
       "      <th>is_alone</th>\n",
       "      <th>has_cabin</th>\n",
       "      <th>is_3rdclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>b</td>\n",
       "      <td>3.62500</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>d</td>\n",
       "      <td>c</td>\n",
       "      <td>35.64165</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Miss</td>\n",
       "      <td>0</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>7.92500</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>C</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>d</td>\n",
       "      <td>c</td>\n",
       "      <td>26.55000</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Mr</td>\n",
       "      <td>0</td>\n",
       "      <td>b</td>\n",
       "      <td>c</td>\n",
       "      <td>8.05000</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass     Sex   Age  SibSp  Parch     Fare Embarked     Deck Title  \\\n",
       "0       3    male  22.0      1      0   7.2500        S  Unknown    Mr   \n",
       "1       1  female  38.0      1      0  71.2833        C        C   Mrs   \n",
       "2       3  female  26.0      0      0   7.9250        S  Unknown  Miss   \n",
       "3       1  female  35.0      1      0  53.1000        S        C   Mrs   \n",
       "4       3    male  35.0      0      0   8.0500        S  Unknown    Mr   \n",
       "\n",
       "   Family_Size Fare_Bin Age_Grp  Fare_Per_Person  Age*Class  Family_Survival  \\\n",
       "0            1        a       b          3.62500       66.0              0.5   \n",
       "1            1        d       c         35.64165       38.0              0.5   \n",
       "2            0        b       b          7.92500       78.0              0.5   \n",
       "3            1        d       c         26.55000       35.0              0.0   \n",
       "4            0        b       c          8.05000      105.0              0.5   \n",
       "\n",
       "   is_alone  has_cabin  is_3rdclass  \n",
       "0         0          0            1  \n",
       "1         0          1            0  \n",
       "2         1          0            1  \n",
       "3         0          1            0  \n",
       "4         1          0            1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y,pred_set,original_train,pred_set_original=get_data()\n",
    "x.info()\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "<h1>MODELLING</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "<p><h3>pipeline_xgb :</h3><ul>\n",
    "    <li></li>\n",
    "    </ul>\n",
    "\n",
    "\n",
    "<p><h3>Pipelines : </h3><ul>\n",
    "    <li>Pipelines act as a reusable container of different transformers that need to be used in a particular order , repeatedly . </li>\n",
    "    <li>Pipelines make the code look cleaner , make the workflow easier , and makes your work reproducible.</li>\n",
    "    <li>Pipelines use a systematic stepwise approach , this approach avoids train-test data contamination .</li>\n",
    "    </ul></p>\n",
    "<p><h4>We will be using 2 different pipelines in this notebook :</h4> <ol>\n",
    "    <li>pipeline_xgb : Pipeline contains XGBClassifier() and  xgboost specific data preprocessing </li>\n",
    "    <li>pipeline_log : Pipeline contains LogisticRegression() and  logistic regression specific data preprocessing </li>\n",
    "    </ol></p>\n",
    "<p><h3>pipeline_xgb : Elements</h3><ol>\n",
    "    <li>Changing 'Pclass' to type : 'object' . </li>\n",
    "    <li>Applying CatBoostEncoder() to all the categorical features .</li>\n",
    "    <li>selecting optimum k features based on chi2 .</li>\n",
    "    <li>containing the classifier .</li>\n",
    "    </ol>\n",
    "<p><h3>pipeline_log : Elements</h3><ol>\n",
    "    <li>Changing 'Pclass' to type : 'object' . </li>\n",
    "    <li>Applying CatBoostEncoder() to all the categorical features .</li>\n",
    "    <li>Applying Quantile transformer to all the numerical features to make them normally distributed . </li>\n",
    "    <li>selecting optimum k features based on chi2 .</li>\n",
    "    <li>containing the classifier .</li>\n",
    "    </ol>\n",
    "\n",
    "<p><h3>pipeline_xgb implemention is given below :</h3><ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 fold cross validation accuracies [0.86516854 0.83707865 0.81460674 0.89325843 0.84745763]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"converting functions to transformers\"\"\"\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "class_cat=FunctionTransformer(class_categorizer)\n",
    "quantile_transformer = FunctionTransformer(quantile_transform)\n",
    "\n",
    "\n",
    "\"\"\"xgboost pipe_line\"\"\"\n",
    "cat_features=['Title', 'Deck' ,'Pclass' , 'Sex' , 'Embarked' , 'Age_Grp' , 'Fare_Bin']\n",
    "num_features = list(x.select_dtypes(include=['int64','float64']).columns)\n",
    "categorical_transformer = pip.Pipeline(steps=[('class_cat',class_cat),\n",
    "                                              ('enc', CatBoostEncoder())\n",
    "                                              ])\n",
    "numerical_transformer = pip.Pipeline([('just','passthrough')])\n",
    "preprocessor = ColumnTransformer(transformers=[('cat', categorical_transformer , cat_features),\n",
    "                                               ('num' , numerical_transformer , num_features)\n",
    "                                               ])\n",
    "pipeline_xgb =pip.Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                  ('feature_select', SelectKBest(chi2 , k = 15)),\n",
    "                                  ('classifier',XGBClassifier(learning_rate=0.01 ,\n",
    "                                                              n_estimators=860,\n",
    "                                                              max_depth=3,\n",
    "                                                              subsample=1,\n",
    "                                                              colsample_bytree=1,\n",
    "                                                              gamma=6,\n",
    "                                                              reg_alpha = 14,\n",
    "                                                              reg_lambda = 3))\n",
    "                                  ])\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"model evaluation\"\"\"\n",
    "cv = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "accuracies = cross_val_score(pipeline_xgb, x , y , cv = cv)\n",
    "print(\"5 fold cross validation accuracies {}\".format(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    " <h4> <p>Parameters of the transformers were already tuned using gridsearchCV .</p><p>Given below is the template used to optimize the hyperparameters of the transformers.</p> </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score : 0.851513997333841  , best params : {'feature_select': SelectKBest(k=15, score_func=<function chi2 at 0x7f1d5db185f0>), 'feature_select__k': 15}  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"Template used for hyperparameter tuning\"\"\"\n",
    "params=[{\n",
    "    # 'classifier__n_estimators' : [i for i in range(700,910,10)]\n",
    "    # 'classifier__subsample' : [i/100 for i in range(80,101)]\n",
    "    'feature_select' : [SelectKBest(chi2)],\n",
    "    'feature_select__k' : [i for i in range(5,19)]\n",
    "    }]\n",
    "\n",
    "cv = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "search=GridSearchCV(estimator=pipeline_xgb,\n",
    "                    param_grid=params,\n",
    "                    n_jobs=-1,\n",
    "                    cv=cv)\n",
    "\n",
    "search.fit(x, y)\n",
    "print(\"best score : {}  , best params : {}  \".format(search.best_score_ , search.best_params_))\n",
    "#search.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "<p><h3>pipeline_log implemention is given below :</h3><ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 fold cross validation accuracies [0.83146067 0.84269663 0.83707865 0.87640449 0.85310734]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"logistic regression pipeline\"\"\"\n",
    "cat_features=['Title', 'Deck' ,'Pclass' , 'Sex' , 'Embarked' , 'Age_Grp' , 'Fare_Bin']\n",
    "num_features = list(x.select_dtypes(include=['int64','float64']).columns)\n",
    "categorical_transformer = pip.Pipeline(steps=[('class_cat',class_cat),\n",
    "                                              ('enc', CatBoostEncoder())\n",
    "                                              ])\n",
    "numerical_transformer = pip.Pipeline([('normal_trans',quantile_transformer)])\n",
    "preprocessor = ColumnTransformer(transformers=[('cat', categorical_transformer , cat_features),\n",
    "                                               ('num' , numerical_transformer , num_features)])\n",
    "pipeline_log =pip.Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                  ('feature_select',SelectKBest(chi2, k = 17 )),\n",
    "                                  ('classifier',LogisticRegression(penalty = 'l2',\n",
    "                                                                   solver = 'liblinear',\n",
    "                                                                   C = 0.25))\n",
    "                                  ])\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"model evaluation\"\"\"\n",
    "cv = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "accuracies = cross_val_score(pipeline_log, x , y , cv = cv)\n",
    "print(\"5 fold cross validation accuracies {}\".format(accuracies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "<p><h1>ENSEMBLE : Weighted Voting classifier</h1></p>\n",
    "<h4><ul>\n",
    "    <li>Voting classifier considers the majority prediction among the predictions of the various classifiers</li>\n",
    "    <li>Classifiers included in ensemble models must be diverse by function and by the features they are trained on for maximum benefit.</li>\n",
    "    </ul></h4>\n",
    "<p><h4>We will use sklearn implementation of weighter voting classifier ( class sklearn.ensemble.VotingClassifier() ) which will contain pipeline_xgb and pipeline_log as its estimators .</h4></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 fold cross validation accuracies [0.85393258 0.85393258 0.83146067 0.89325843 0.84180791]\n"
     ]
    }
   ],
   "source": [
    "classifier = VotingClassifier(estimators=[('XGB', pipeline_xgb), ('LOG', pipeline_log)])\n",
    "\n",
    "\"\"\"model evaluation\"\"\"\n",
    "cv = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "accuracies = cross_val_score(classifier, x , y , cv = cv)\n",
    "print(\"5 fold cross validation accuracies {}\".format(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "<p><h4>We see that the weighted voting classifier is more stable than its estimators when they are seperate .</h4></p>\n",
    "<p><h4>We will now proceed to making our submition using our ensemble model .</h4></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(x,y)\n",
    "y_submit=pd.Series((classifier.predict(pred_set)))\n",
    "y_submit=y_submit.astype(int)\n",
    "y_1=pred_set_original.PassengerId\n",
    "y_submit_f=pd.concat([y_1,y_submit],axis=1)\n",
    "y_submit_f.rename( columns={ 0 :'Survived'}, inplace=True )\n",
    "y_submit_f.to_csv('submission.csv',index=False)        "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "<h3> hope this helps all the readers  .  if u like this notebook , give an upvote to keep me motivated  . critical comments are appreciated  , cheers :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
